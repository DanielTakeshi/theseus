{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Creating Custom Cost Functions</h1>\n",
    "\n",
    "In this tutorial, we show how to create a custom cost function that might be needed for an application. While we can always use the `AutoDiffCostFunction` by simply writing an error function, it is often more efficient for compute-intensive applications to derive a new `CostFunction` subclass and use closed-form Jacobians. \n",
    "\n",
    "We will show how to write a custom `VectorDifference` cost function in this tutorial. This cost function provides the difference between two `Vector`s as the error. \n",
    "\n",
    "Note: `VectorDifference` is a simplified version of the `Difference` cost function already provided in the Theseus library, and shown in Tutorial 0. `Difference` can be used on any LieGroup, while `VectorDifference` can only be used on Vectors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Initialization</h2> \n",
    "\n",
    "Any `CostFunction` subclass should be initialized with a `CostWeight` and all arguments needed to compute the cost function. In this example, we set up `__init__` function for `VectorDifference` to require as input the two `Vector`s whose difference we wish to compute: the `Vector` to be optimized, `var`, and the `Vector` that is the reference for comparison, `target`. \n",
    "\n",
    "In addition, the `__init__` function also needs to register the optimization variables and all the auxiliary variables. In this example, optimization variable `var` is registered with `register_optim_vars`. The other input necessary to evaluate the cost, `target` is registered with `register_aux_vars`. This is required for the nonlinear optimizers to work correctly: these functions register the optimization and auxiliary variables into internal lists, and then are easily used by the relevant `Objective` to add them, ensure no name collisions, and to update them with new values.\n",
    "\n",
    "The `CostWeight` is used to weight the errors and jacobians, and is required by every `CostFunction` sub-class (the error and jacobian weighting functions are inherited from the parent `CostFunction` class.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Tuple\n",
    "import theseus as th\n",
    "\n",
    "class VectorDifference(th.CostFunction):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cost_weight: th.CostWeight,\n",
    "        var: th.Vector,\n",
    "        target: th.Vector,\n",
    "        name: Optional[str] = None,\n",
    "    ):\n",
    "        super().__init__(cost_weight, name=name) \n",
    "\n",
    "        # add checks to ensure the input arguments are of the same class and dof:\n",
    "        if not isinstance(var, target.__class__):\n",
    "            raise ValueError(\n",
    "                \"Variable for the VectorDifference inconsistent with the given target.\"\n",
    "            )\n",
    "        if not var.dof() == target.dof():\n",
    "            raise ValueError(\n",
    "                \"Variable and target in the VectorDifference must have identical dof.\"\n",
    "            )\n",
    "\n",
    "        self.var = var\n",
    "        self.target = target\n",
    "\n",
    "        # register variable and target\n",
    "        self.register_optim_vars([\"var\"])\n",
    "        self.register_aux_vars([\"target\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Implement abstract functions</h2> \n",
    "\n",
    "Next, we need to implement the abstract functions of `CostFunction`: `dim`, `error`, `jacobians`, and `_copy_impl`:\n",
    "- `dim`: returns the degrees of freedom (`dof`) of the error; in this case, this is the `dof` of the optimization variable `var`\n",
    "- `error`: returns the difference of Vectors i.e. `var` - `target`\n",
    "- `jacobian`: returns the Jacobian of the error with respect to the `var`\n",
    "- `_copy_impl`: creates a deep copy of the internal class members\n",
    "\n",
    "We illustrate these below (including once again the `__init__` function from above, so the class is fully defined.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "class VectorDifference(th.CostFunction):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cost_weight: th.CostWeight,\n",
    "        var: th.Vector,\n",
    "        target: th.Vector,\n",
    "        name: Optional[str] = None,\n",
    "    ):\n",
    "        super().__init__(cost_weight, name=name) \n",
    "        self.var = var\n",
    "        self.target = target\n",
    "        # to improve readability, we have skipped the data checks from code block above\n",
    "        self.register_optim_vars([\"var\"])\n",
    "        self.register_aux_vars([\"target\"])\n",
    "\n",
    "    def error(self) -> torch.Tensor:\n",
    "        return (self.var - self.target).tensor\n",
    "\n",
    "    def jacobians(self) -> Tuple[List[torch.Tensor], torch.Tensor]:\n",
    "        return [\n",
    "            # jacobian of error function wrt var is identity matrix I\n",
    "            torch.eye(self.dim(), dtype=self.var.dtype)  \n",
    "            # repeat jacobian across each element in the batch\n",
    "            .repeat(self.var.shape[0], 1, 1)  \n",
    "            # send to variable device\n",
    "            .to(self.var.device)  \n",
    "        ], self.error()\n",
    "\n",
    "    def dim(self) -> int:\n",
    "        return self.var.dof()\n",
    "\n",
    "    def _copy_impl(self, new_name: Optional[str] = None) -> \"VectorDifference\":\n",
    "        return VectorDifference(  # type: ignore\n",
    "            self.var.copy(), self.weight.copy(), self.target.copy(), name=new_name\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation of the Above\n",
    "\n",
    "Why is the Jacobian like this? We have `var` and `target` but let's rewrite this in math notation to make it simpler. First, Jacobians need to operate on vector-valued functions, here we'll consider 2D examples, taking in a 2D vector and producing a 2D output. What is a bit confusing is that our function $f$ is taking in TWO 2D vectors, but we'll only differentiate with respect to one of them. Let's write $f$ as a function of two vectors as follows, where the $x$s indicate the `var` from above, and $y$s indicate the `target`.\n",
    "$$\n",
    "f( \\langle x_1, x_2 \\rangle, \\langle y_1, y_2 \\rangle) = \\langle x_1 - y_1, x_2 - y_2 \\rangle\n",
    "$$\n",
    "\n",
    "Technically, we have $f : (\\mathbb{R}^2 \\times \\mathbb{R}^2) \\to \\mathbb{R}^2$. But if we want the derivative with respect to one vector then I think we can consider that to be the Jacobian which is suggested by the comments:\n",
    "\n",
    "$$\n",
    "J = \\frac{\\partial f}{\\partial \\langle x_1, x_2 \\rangle} = \\frac{\\partial f}{\\partial \\langle x_1, x_2 \\rangle} \\langle x_1 - y_1, x_2 - y_2 \\rangle\n",
    "$$\n",
    "\n",
    "The first row is: $\\langle \\frac{\\partial f_1}{\\partial x_1}, \\frac{\\partial f_1}{\\partial x_2} \\rangle = \\langle \\frac{\\partial}{\\partial x_1} (x_1 - y_1), \\frac{\\partial}{\\partial x_2} (x_1 - y_1) \\rangle = \\langle 1, 0 \\rangle$\n",
    "\n",
    "The second row is: $\\langle \\frac{\\partial f_2}{\\partial x_1} \\frac{\\partial f_2}{\\partial x_2} \\rangle  = \\langle \\frac{\\partial}{\\partial x_1}(x_2-y_2), \\frac{\\partial}{\\partial x_2} (x_2-y_2) \\rangle = \\langle 0, 1 \\rangle$\n",
    "\n",
    "Is that the way to think about it? The easiest way to adjust would be if there are some scalars we can use to multiply stuff, and ideally those are also in the Jacobian, so for example in the `error` function if we do `2. * self.var` then the Jacobian would be basically $2 \\cdot I$. I think the reason why I didn't notice this earlier when I was fiddling around with Tutorial 00 is that `cf.jacobians` uses the internal Jacobian computed from the code and that is something fixed (and in any case we were not modifying the internal `error` function in that tutorial).\n",
    "\n",
    "I am not sure if there is an automated way to check if the Jacobian is correctly computed. Edit: I should have checked, I am sure this will do it: https://github.com/facebookresearch/theseus/issues/323"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Usage</h2>\n",
    "\n",
    "We show now that the `VectorDifference` cost function works as expected. \n",
    "\n",
    "For this, we create a set of `VectorDifference` cost functions each over a pair of `Vector`s <i>a_i</i> and <i>b_i</i>, and add them to an `Objective`. We then create the data for each `Vector` <i>a_i</i> and <i>b_i</i> of the `VectorDifference` cost functions, and `update` the `Objective` with it. The code snippet below shows that the `Objective` error is correctly computed.\n",
    "\n",
    "We use a `ScaleCostWeight` as the input `CostWeight` here: this is a scalar real-valued `CostWeight` used to weight the `CostFunction`; for simplicity we use a fixed value of 1. in this example.\n",
    "\n",
    "\n",
    "Daniel: for some reason now `objective.error_squared_norm()` is not fixing things?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected type 'Vector' or 'Tensor', got int",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m objective\u001b[39m.\u001b[39mupdate(theseus_inputs)\n\u001b[1;32m     19\u001b[0m \u001b[39m# sum of squares of errors [1, 1] for 10 cost fns: the result should be 20\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m error_sq \u001b[39m=\u001b[39m objective\u001b[39m.\u001b[39;49merror_squared_norm()  \u001b[39m# Daniel the usual fix.\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSample error squared norm: \u001b[39m\u001b[39m{\u001b[39;00merror_sq\u001b[39m.\u001b[39mitem()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/theseus/lib/python3.8/site-packages/theseus/core/objective.py:434\u001b[0m, in \u001b[0;36mObjective.error_squared_norm\u001b[0;34m(self, input_tensors, also_update)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39merror_squared_norm\u001b[39m(\n\u001b[1;32m    429\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    430\u001b[0m     input_tensors: Optional[Dict[\u001b[39mstr\u001b[39m, torch\u001b[39m.\u001b[39mTensor]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    431\u001b[0m     also_update: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    432\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m    433\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 434\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merror(input_tensors\u001b[39m=\u001b[39;49minput_tensors, also_update\u001b[39m=\u001b[39;49malso_update) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m    435\u001b[0m     )\u001b[39m.\u001b[39msum(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/theseus/lib/python3.8/site-packages/theseus/core/objective.py:417\u001b[0m, in \u001b[0;36mObjective.error\u001b[0;34m(self, input_tensors, also_update)\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate(input_tensors\u001b[39m=\u001b[39minput_tensors, _update_vectorization\u001b[39m=\u001b[39malso_update)\n\u001b[1;32m    409\u001b[0m \u001b[39m# Current behavior when vectorization is on, is to always compute the error.\u001b[39;00m\n\u001b[1;32m    410\u001b[0m \u001b[39m# One could potentially optimize by only recompute when `input_tensors`` is\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[39m# not None, and serving from the jacobians cache. However, when robust cost\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[39m# by passing `input_tensors`, so for optimizers the current version should be\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[39m# good enough.\u001b[39;00m\n\u001b[1;32m    416\u001b[0m error_vector \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(\n\u001b[0;32m--> 417\u001b[0m     [cf\u001b[39m.\u001b[39mweighted_error() \u001b[39mfor\u001b[39;00m cf \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_error_iter()], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m    418\u001b[0m )\n\u001b[1;32m    420\u001b[0m \u001b[39mif\u001b[39;00m input_tensors \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m also_update:\n\u001b[1;32m    421\u001b[0m     \u001b[39m# This line reverts back to the old tensors if a persistent update wasn't\u001b[39;00m\n\u001b[1;32m    422\u001b[0m     \u001b[39m# required (i.e., `also_update is False`).\u001b[39;00m\n\u001b[1;32m    423\u001b[0m     \u001b[39m# In this case, we pass _update_vectorization=False because\u001b[39;00m\n\u001b[1;32m    424\u001b[0m     \u001b[39m# vectorization wasn't updated in the first call to `update()`.\u001b[39;00m\n\u001b[1;32m    425\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate(old_tensors, _update_vectorization\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/theseus/lib/python3.8/site-packages/theseus/core/objective.py:417\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate(input_tensors\u001b[39m=\u001b[39minput_tensors, _update_vectorization\u001b[39m=\u001b[39malso_update)\n\u001b[1;32m    409\u001b[0m \u001b[39m# Current behavior when vectorization is on, is to always compute the error.\u001b[39;00m\n\u001b[1;32m    410\u001b[0m \u001b[39m# One could potentially optimize by only recompute when `input_tensors`` is\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[39m# not None, and serving from the jacobians cache. However, when robust cost\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[39m# by passing `input_tensors`, so for optimizers the current version should be\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[39m# good enough.\u001b[39;00m\n\u001b[1;32m    416\u001b[0m error_vector \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(\n\u001b[0;32m--> 417\u001b[0m     [cf\u001b[39m.\u001b[39;49mweighted_error() \u001b[39mfor\u001b[39;00m cf \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_error_iter()], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m    418\u001b[0m )\n\u001b[1;32m    420\u001b[0m \u001b[39mif\u001b[39;00m input_tensors \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m also_update:\n\u001b[1;32m    421\u001b[0m     \u001b[39m# This line reverts back to the old tensors if a persistent update wasn't\u001b[39;00m\n\u001b[1;32m    422\u001b[0m     \u001b[39m# required (i.e., `also_update is False`).\u001b[39;00m\n\u001b[1;32m    423\u001b[0m     \u001b[39m# In this case, we pass _update_vectorization=False because\u001b[39;00m\n\u001b[1;32m    424\u001b[0m     \u001b[39m# vectorization wasn't updated in the first call to `update()`.\u001b[39;00m\n\u001b[1;32m    425\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate(old_tensors, _update_vectorization\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/theseus/lib/python3.8/site-packages/theseus/core/cost_function.py:98\u001b[0m, in \u001b[0;36mCostFunction.weighted_error\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mweighted_error\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m---> 98\u001b[0m     error \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merror()\n\u001b[1;32m     99\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mweight_error(error)\n",
      "Cell \u001b[0;32mIn[4], line 19\u001b[0m, in \u001b[0;36mVectorDifference.error\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39merror\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m---> 19\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39m2\u001b[39;49m \u001b[39m*\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvar \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget)\u001b[39m.\u001b[39mtensor\n",
      "File \u001b[0;32m~/miniconda3/envs/theseus/lib/python3.8/site-packages/theseus/geometry/vector.py:109\u001b[0m, in \u001b[0;36mVector.__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m(tensor\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mmul(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtensor, other))\n\u001b[1;32m    108\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 109\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    110\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mexpected type \u001b[39m\u001b[39m'\u001b[39m\u001b[39mVector\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mTensor\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(other)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    111\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: expected type 'Vector' or 'Tensor', got int"
     ]
    }
   ],
   "source": [
    "cost_weight = th.ScaleCostWeight(1.0)\n",
    "\n",
    "# construct cost functions and add to objective\n",
    "objective = th.Objective()\n",
    "num_test_fns = 10\n",
    "for i in range(num_test_fns):\n",
    "    a = th.Vector(2, name=f\"a_{i}\")\n",
    "    b = th.Vector(2, name=f\"b_{i}\")\n",
    "    cost_fn = VectorDifference(cost_weight, a, b)\n",
    "    objective.add(cost_fn)\n",
    "    \n",
    "# create data for adding to the objective\n",
    "theseus_inputs = {}\n",
    "for i in range(num_test_fns):\n",
    "    # each pair of var/target has a difference of [1, 1]\n",
    "    theseus_inputs.update({f\"a_{i}\": torch.ones((1,2)), f\"b_{i}\": 2 * torch.ones((1,2))})\n",
    "\n",
    "objective.update(theseus_inputs)\n",
    "# sum of squares of errors [1, 1] for 10 cost fns: the result should be 20\n",
    "error_sq = objective.error_squared_norm()  # Daniel the usual fix.\n",
    "print(f\"Sample error squared norm: {error_sq.item()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Jacobians\n",
    "\n",
    "(New section added by Daniel Seita)\n",
    "\n",
    "Let's try and modify the cost functions and then check if the Jacobians will work as expected.\n",
    "\n",
    "Edit: wait that function was added on March 08, 2023. Yet from PyPI, 0.1.4 (which is what I am using) is from January 19, 2023. https://pypi.org/project/theseus-ai/#history  We need to update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'check_jacobians' from 'theseus.utils.utils' (/home/seita/miniconda3/envs/theseus/lib/python3.8/site-packages/theseus/utils/utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtheseus\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m check_jacobians\n\u001b[1;32m      3\u001b[0m check_jacobians(cf\u001b[39m=\u001b[39mobjective, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'check_jacobians' from 'theseus.utils.utils' (/home/seita/miniconda3/envs/theseus/lib/python3.8/site-packages/theseus/utils/utils.py)"
     ]
    }
   ],
   "source": [
    "from theseus.utils.utils import check_jacobians\n",
    "\n",
    "check_jacobians(cf=objective, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "theseus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
